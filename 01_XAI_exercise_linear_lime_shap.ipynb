{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjGV-I0T0vwz"
      },
      "source": [
        "# Exercise 01\n",
        "\n",
        "In this exercise we try to cover linear models, LIME and SHAP analysis.\n",
        "We import our required libraries and set random seeds for reproducability.\n",
        "\n",
        "## Dataset Preparation\n",
        "\n",
        "First we fetch the [California Housing Dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) and take a first look at the data. The function returns two [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) objects (think mix of Excel and NumPy Arrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0islRtsl0qtI"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import tqdm\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "features, labels = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC63qQbXxBO7"
      },
      "source": [
        "In many cases, it can already be helpful to perform a visual inspection of our dataset. This helps us identify first patterns, clusters, or malformed distributions, indicating the need for feature normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDH9-GI01vQx"
      },
      "outputs": [],
      "source": [
        "plt.scatter(features[\"MedInc\"], features[\"HouseAge\"], c=labels, alpha=0.3)\n",
        "plt.xlabel(\"Median Income (100k $)\")\n",
        "plt.ylabel(\"House Age\")\n",
        "cbar = plt.colorbar()\n",
        "cbar.ax.set_ylabel(\"Median House Value (100k $)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3s1kPicxX6x"
      },
      "source": [
        "We perform a train- and test-split of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CZVni_83cwn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "    features, labels, test_size=0.33, random_state=42\n",
        ")\n",
        "print(train_features.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swAeaQ3eyUms"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "We use PyTorch do define our own Linear Regression Model.\n",
        "Our Implementation allows us to define our training objective afterwards, allowing different training regimes.\n",
        "\n",
        "*Note that we have a bug somewhere, which is why the results are bad and the learning rate is stupid.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmudyXg6y96g"
      },
      "outputs": [],
      "source": [
        "class LinearRegressor(torch.nn.Module):\n",
        "    def __init__(self, num_features: int, num_outputs: int) -> None:\n",
        "        \"\"\"Simple Linear Regressor in Torch.\n",
        "        Args:\n",
        "            num_features (int): number of feature dimensions\n",
        "            num_outputs (int): number of output dimensions\n",
        "        \"\"\"\n",
        "\n",
        "        torch.manual_seed(42)\n",
        "        np.random.seed(42)\n",
        "\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(num_features, num_outputs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Network forward function\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input Tensor (BATCH x FEATURES)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: model prediction (BATCH x OUTPUT)\n",
        "        \"\"\"\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "    def weights(self) -> np.ndarray:\n",
        "        \"\"\"get regressor weights\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: model weights (FEATURES,)\n",
        "        \"\"\"\n",
        "        return self.linear.weight.detach().numpy().squeeze()\n",
        "\n",
        "    def predict(self, x: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"make model prediction outside of torch\n",
        "\n",
        "        Args:\n",
        "            x (pd.DataFrame): input dataframe (BATCH x FEATURES)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: model prediction (BATCH x OUTPUT)\n",
        "        \"\"\"\n",
        "        tensor = torch.tensor(x.values, dtype=torch.float32)\n",
        "        pred = self.forward(tensor)\n",
        "        return pred.detach().numpy()\n",
        "\n",
        "\n",
        "def optimize_lin_reg(model, loss_fn, epochs=2, tf=None):\n",
        "    \"\"\"Training Loop\n",
        "\n",
        "    Args:\n",
        "        model: torch model\n",
        "        loss_fn: callable function as loss / optimization criterion\n",
        "        epochs (int, optional): number of training epochs Defaults to 2.\n",
        "        tf (pd.DataFrame, optional): Training Feature Set. If None, take global train_features. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple: trained model, array of loss values\n",
        "    \"\"\"\n",
        "\n",
        "    if tf is None:\n",
        "        tf = train_features\n",
        "    # SGD is not the ideal choice - see ridiculous learning rate - No Time to do it right.\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-7)\n",
        "\n",
        "    feature_tensor = torch.tensor(tf.values, dtype=torch.float32)[:]\n",
        "    label_tensor = torch.tensor(train_labels.values, dtype=torch.float32)[:, None]\n",
        "    losses = []\n",
        "\n",
        "    for _ in tqdm.tqdm(range(epochs)):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model.forward(feature_tensor)\n",
        "        loss = loss_fn(label_tensor, preds, model.linear.weight)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3CsWtG8DbiP"
      },
      "source": [
        "# Task 01: Linear Regression Objectives\n",
        "Linear Regression can be trained with different objective functions.\n",
        "\n",
        "### 1.1 Ordinary Least Squares:\n",
        "Complete the function stub for ordinary least squares objective as discussed in the lecture. *As this function is used by the torch optimizer, only use torch functions, no numpy.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "839YACMFZqX1"
      },
      "outputs": [],
      "source": [
        "def ordinary_least_squares(\n",
        "    true_y: torch.Tensor, pred_y: torch.Tensor, weights: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"ordinary least squares target / loss function\n",
        "\n",
        "    Args:\n",
        "        true_y (torch.Tensor): actual target values (BATCH x 1)\n",
        "        pred_y (torch.Tensor): predicted values (BATCH x 1)\n",
        "        weights (torch.Tensor): model weights (BATCH x FEATURES). Might not be required\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Loss Value (1,). Basically a float\n",
        "    \"\"\"\n",
        "\n",
        "    YOUR CODE GOES HERE:\n",
        "    res = 0\n",
        "\n",
        "    return torch.mean(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rebUT_cVZqX1"
      },
      "source": [
        "### 1.2 Ridge Regression:\n",
        "Complete the function stub for the ridge regression objective as discussed in the lecture. *As this function is used by the torch optimizer, only use torch functions, no numpy.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmAqfvWvE94M"
      },
      "outputs": [],
      "source": [
        "def ridge(\n",
        "    true_y: torch.Tensor, pred_y: torch.Tensor, weights: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"ridge regression target / loss function\n",
        "\n",
        "    Args:\n",
        "        true_y (torch.Tensor): actual target values (BATCH x 1)\n",
        "        pred_y (torch.Tensor): predicted values (BATCH x 1)\n",
        "        weights (torch.Tensor): model weights (BATCH x FEATURES). Might not be required\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Loss Value (1,). Basically a float\n",
        "    \"\"\"\n",
        "    alpha = 1000000.0  # DO NOT CHANGE!!\n",
        "\n",
        "    YOUR CODE GOES HERE\n",
        "    res = 0\n",
        "\n",
        "    return torch.mean(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvoSEXE7ZqX2"
      },
      "source": [
        "### 1.3 R2 Score\n",
        "The R2 Score is a metric to estimate model fit to the data. Complete the function stub for the r2 score function.\n",
        "For simplicities sake, use **only** numpy functions, no torch!! (Sorry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvCHfFz3BSZh"
      },
      "outputs": [],
      "source": [
        "def r2_score(pred: np.ndarray, target: np.ndarray) -> float:\n",
        "    \"\"\"R2 metric function\n",
        "\n",
        "    Args:\n",
        "        pred (np.ndarray): Numpy array of predictions (BATCH x 1)\n",
        "        target (np.ndarray): Numpy array of target values (BATCH x 1)\n",
        "\n",
        "    Returns:\n",
        "        float: r2 score\n",
        "    \"\"\"\n",
        "\n",
        "    YOUR CODE GOES HERE:\n",
        "    res = 0\n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcJ8_sBFZqX2"
      },
      "source": [
        "## Model Evaluation\n",
        "We define some helpful functions for model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mIMXzDaZqX2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "def eval(model, loss: list, tf=None):\n",
        "    \"\"\"create some evaluation plots\n",
        "\n",
        "    Args:\n",
        "        model: regression model\n",
        "        loss (list): list of loss values\n",
        "        tf (pd.DataFrame, optional): Test Feature Set. If None, take global test_features. Defaults to None.\n",
        "    \"\"\"\n",
        "    if tf is None:\n",
        "        tf = test_features\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "    # Loss Curve\n",
        "    axs[0].plot(np.log(loss))\n",
        "    axs[0].set_ylabel(\"Log-Loss\")\n",
        "\n",
        "    # Weight Plot\n",
        "    axs[1].bar(tf.columns, model.weights())\n",
        "    axs[1].tick_params(axis=\"x\", labelrotation=45)\n",
        "\n",
        "    # MSE and R2 Score\n",
        "    pred = model.predict(tf)\n",
        "    mse = mean_squared_error(test_labels, pred)\n",
        "    r2 = r2_score(pred.squeeze(), test_labels)\n",
        "    fig.suptitle(f\"MSE: {mse:.3f} R2: {r2:.3f}\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l486iFgc2BG2"
      },
      "source": [
        "First we train a normal Linear Regression Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69KdyYpCC3Vv"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegressor(train_features.shape[1], 1)\n",
        "lr, loss = optimize_lin_reg(lr, ordinary_least_squares, epochs=100)\n",
        "\n",
        "eval(lr, loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaz8a0Ny2BG2"
      },
      "source": [
        "We can see the influence of the model, when we use the ridge regression target function:\n",
        "\n",
        "Note the values on the weight's y-axis. The regularizer is so dominating, that the weights are all basically zero.\n",
        "This is intentional to give the greatest visual change in the plot compared to the one above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT1PE5axt0f2"
      },
      "outputs": [],
      "source": [
        "ridge_lr = LinearRegressor(train_features.shape[1], 1)\n",
        "ridge_lr, loss = optimize_lin_reg(ridge_lr, ridge, epochs=100)\n",
        "\n",
        "eval(ridge_lr, loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ORtGooV2BG2"
      },
      "source": [
        "## Question: How would you rate both Model's overall Performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGjmUZ65-fae"
      },
      "source": [
        "# Additional Info: Regularization\n",
        "\n",
        "We now construct an example to show the advantage of regularization. First, we create a corrupted training set, where the label is part of the features. This should lead to a model which focuses only on one feature.\n",
        "\n",
        "As our own pytorch LR model is buggy, we will use the scikit-learn models to demonstrate the purpose of regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWgLc0znZqX2"
      },
      "outputs": [],
      "source": [
        "corrupted_train_features = pd.concat([train_features, train_labels], axis=1)\n",
        "plt.scatter(corrupted_train_features.iloc[:, -1], train_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA9_-VdrZqX2"
      },
      "source": [
        "The unregularized regression model overfits on the corrupting feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti0-LfsMZqX2"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "lr = linear_model.LinearRegression()\n",
        "lr.fit(corrupted_train_features, train_labels)\n",
        "\n",
        "plt.bar(corrupted_train_features.columns, lr.coef_)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0pDG6-6ZqX2"
      },
      "source": [
        "By using Ridge regression, this behaviour is punished. The other features wil also be considered:\n",
        "\n",
        "(Even though because of the perfect fit, we have to use a very high alpha value.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jem8_FXEZqX2"
      },
      "outputs": [],
      "source": [
        "lr = linear_model.Ridge(alpha=1000000)\n",
        "lr.fit(corrupted_train_features, train_labels)\n",
        "\n",
        "plt.bar(corrupted_train_features.columns, lr.coef_)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcJEkM8Du8HN"
      },
      "source": [
        "# Task 02: LIME\n",
        "\n",
        "LIME is a posthoc analyis method for explaining complex models. To showcase lime, we first define our \"deep\" neural network and train it on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "317ws2_Vu9hG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class ComplexNetwork(torch.nn.Module):\n",
        "    \"\"\"A more Complex neural network with hidden layers.\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, num_outputs):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(42)\n",
        "        np.random.seed(42)\n",
        "        self.net = torch.nn.Sequential(\n",
        "            torch.nn.Linear(num_features, num_features * 4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(num_features * 4, num_features * 4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(num_features * 4, num_features * 4),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(num_features * 4, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def predict(self, x_array: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"make model prediction outside of torch\n",
        "\n",
        "        Args:\n",
        "            x_array (np.ndarray): input array (BATCH x FEATURES)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: model prediction (BATCH x OUTPUT)\n",
        "        \"\"\"\n",
        "        x_tensor = torch.tensor(x_array, dtype=torch.float32)\n",
        "        return self.forward(x_tensor).detach().numpy()\n",
        "\n",
        "\n",
        "def train_net(model, epochs=5, tf=None):\n",
        "    \"\"\"Training Loop\n",
        "\n",
        "    Args:\n",
        "        model: torch model\n",
        "        epochs (int, optional): number of training epochs Defaults to 2.\n",
        "        tf (pd.DataFrame, optional): Training Feature Set. If None, take global train_features. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple: trained model, array of loss values\n",
        "    \"\"\"\n",
        "\n",
        "    if tf is None:\n",
        "        tf = train_features\n",
        "    # SGD is not the ideal choice - see ridiculous learning rate - No Time to do it right.\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.000001)\n",
        "\n",
        "    feature_tensor = torch.tensor(tf.values, dtype=torch.float32)[:]\n",
        "    label_tensor = torch.tensor(train_labels.values, dtype=torch.float32)[:, None]\n",
        "    losses = []\n",
        "\n",
        "    for _ in tqdm.tqdm(range(epochs)):\n",
        "        optimizer.zero_grad()\n",
        "        preds = model.forward(feature_tensor)\n",
        "        loss = torch.nn.functional.mse_loss(preds, label_tensor)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model, losses\n",
        "\n",
        "\n",
        "cn = ComplexNetwork(train_features.shape[1], 1)\n",
        "cn, loss = train_net(cn)\n",
        "plt.plot(loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUcjIkarZqX3"
      },
      "source": [
        "### 2.1 Data Pertubation\n",
        "\n",
        "We now need a function to get samples from the local neighborhood of our sample of interest.\n",
        "Write a function, which takes an array of identical copies of a point of interest (poi) and the standard deviation of all features, to return an array with perturbed data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4J6c8DXZqX3"
      },
      "outputs": [],
      "source": [
        "poi = test_features.iloc[50].values[None, :]\n",
        "pois = np.repeat(poi, 100, axis=0)\n",
        "print(\"POIs SHAPE\", pois.shape)\n",
        "\n",
        "std = train_features.std()\n",
        "std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuUU01WJZqX3"
      },
      "outputs": [],
      "source": [
        "def perturb(pois: np.ndarray, std: np.ndarray):\n",
        "    \"\"\"Perturbs an array of pois\n",
        "\n",
        "    Args:\n",
        "        pois (np.ndarray): points of interest (BATCH x FEATURES)\n",
        "        std (np.ndarray): array of std values (1 x FEATURES)\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    noise = np.random.randn(*pois.shape)\n",
        "\n",
        "    YOUR CODE GOES HERE:\n",
        "    new_pois = pois\n",
        "\n",
        "    return new_pois\n",
        "\n",
        "\n",
        "perturbed_features = perturb(pois, std.values[None, :])\n",
        "perturbed_labels = cn.predict(perturbed_features).squeeze()\n",
        "print(\"MEAN PERTUBATION PER FEATURE: \", np.mean(perturbed_features, axis=0) - poi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpjMzrwCZqX3"
      },
      "source": [
        "## 2.2 Surrogate Model\n",
        "Now we need to train an interpretable surrogate model on our perturbed dataset `(perturbed_features, perturbed_labels)`. The sections above should hold all the information you need to be able get to an explanation. We recommend you use scikit-learn models and not our own pytorch implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1NzQoidZqX3"
      },
      "outputs": [],
      "source": [
        "lr = linear_model.LinearRegression()\n",
        "lr.fit(perturbed_features, perturbed_labels)\n",
        "\n",
        "plt.bar(train_features.columns, lr.coef_)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7vQpjmsZqX3"
      },
      "source": [
        "# SHAP Examples\n",
        "\n",
        "SHAP and shapley values are some of the most widely used explanation methods at the moment. We now explore some visualization examples provided by the python library `shap`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSjv6Dptb5vn"
      },
      "outputs": [],
      "source": [
        "!pip install shap -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuCCRDL8eivM"
      },
      "source": [
        "The [Beeswarm](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html) plot is very information dense. Each dot represents a sample, with the SHAP value (impact) on the X axis, the feature value encoded by color and the piling up along y signifying density.\n",
        "\n",
        "This is a global model explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUNcl1w9ZqX3"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "model = MLPRegressor(hidden_layer_sizes=[32, 100, 32])\n",
        "model.fit(train_features, train_labels)\n",
        "\n",
        "explainer = shap.Explainer(model.predict, train_features.iloc[:200])\n",
        "shap_values = explainer(train_features.iloc[:200])\n",
        "\n",
        "shap.plots.beeswarm(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbOifRQ9eKD0"
      },
      "source": [
        "The [Heatmap Visualization](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/heatmap.html) sorts the outputs by the model value (line at the top, left to right) and shows the impact of each sample instance. The color is now the SHAP value, not the feature value!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgKhee0fZqX3"
      },
      "outputs": [],
      "source": [
        "shap.plots.heatmap(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjV63IxRd7an"
      },
      "source": [
        "The SHAP Waterfall plot is designed for explaining the \"decision\" process for a single example. It is a local explanation method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTEZyQ8TdSTl"
      },
      "outputs": [],
      "source": [
        "shap.plots.waterfall(shap_values[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}