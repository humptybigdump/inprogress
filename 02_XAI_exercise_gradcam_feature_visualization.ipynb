{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fZtpdxRqTE63",
        "aeCsOHYVU-lk",
        "_bCrSrOugwM-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 02 - Neural Network Features with CV\n",
        "\n",
        "In this exercise we want to demonstrate the discussed methods for analysing deep neural networks, especially for Computer Vision.\n",
        "\n",
        "We start by importing our dependencies and create a pretrained ResNet50"
      ],
      "metadata": {
        "id": "fZtpdxRqTE63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAfu_p5lHdsa"
      },
      "outputs": [],
      "source": [
        "### THIS CELL CAN TAKE UP TO 30s TO EXECUTE\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=weights)\n",
        "\n",
        "# We need the preprocess function to transform images into the input range the network expects\n",
        "preprocess = weights.transforms()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load a test image from the web. Feel free to test different images"
      ],
      "metadata": {
        "id": "q9HFUPdNToj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"\n",
        "test_image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "plt.imshow(test_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T4ygp55eIFNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can verify the pretrained network by letting it predict the top *n* classes, such as this:"
      ],
      "metadata": {
        "id": "telUb12wVE35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_as_batch(img: Image):\n",
        "    img_transformed = preprocess(test_image)\n",
        "    batch = torch.unsqueeze(img_transformed, 0)\n",
        "    return batch\n",
        "\n",
        "\n",
        "def predict_model(img: Image, m=model):\n",
        "    batch = load_as_batch(img)\n",
        "    prediction = m(batch).squeeze(0).softmax(0)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def top_n_classes(prediction: torch.Tensor, n: int = 5):\n",
        "    classes = torch.topk(prediction, n).indices.tolist()\n",
        "    return classes\n",
        "\n",
        "\n",
        "def print_class_predicitons(prediction, top_n=5):\n",
        "    classes = top_n_classes(prediction, n=top_n)\n",
        "    for class_id in classes:\n",
        "        score = prediction[class_id].item()\n",
        "        category_name = weights.meta[\"categories\"][class_id]\n",
        "        print(f\"{category_name}: {100 * score:.1f}%\")\n",
        "\n",
        "\n",
        "prediction = predict_model(test_image)\n",
        "print_class_predicitons(prediction, top_n=5)"
      ],
      "metadata": {
        "id": "BtLatLXdJK8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GradCAM\n",
        "\n",
        "To compute the GradCAM method, we require the gradient activations of our (last) convolution layer.\n",
        "\n",
        "To get an overview of our model architecture, we use a simple `print()` statement.\n",
        "\n",
        "For the Resnet50 model, the last two blocks are a pooling layer, and the fully connected output layer. Therefore, up to everything but the last two modules are our convolution activations. Or in code: `list(model.children())[:-2]`\n",
        "\n",
        "When using different architectures, this must be adapted!"
      ],
      "metadata": {
        "id": "aeCsOHYVU-lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Network Architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "goAs_BXJKuuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use this insight to write a Wrapper around the ResNet50 Model, to acces the gradients."
      ],
      "metadata": {
        "id": "cXMjy-kmWduu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetWrapper(torch.nn.Module):\n",
        "    \"\"\"Adapted from https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = resnet50(weights=weights)\n",
        "        # Resnet50 specific: Everything up until -2 is Convolution\n",
        "        self.features_conv = torch.nn.Sequential(*list(self.model.children())[:-2])\n",
        "        self.gradients = None\n",
        "\n",
        "    def activations_hook(self, grad):\n",
        "        # The Hook gets executed during .backward(), when computing the gradients.\n",
        "        # It receives the gradient as input, and saves it to the attribute.\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform Comvolution\n",
        "        x = self.features_conv(x)\n",
        "        # Register the Hook\n",
        "        h = x.register_hook(self.activations_hook)\n",
        "\n",
        "        # Perform the rest of the model as it normally would\n",
        "        x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.model.fc(x)\n",
        "        return x\n",
        "\n",
        "    def get_activations_gradient(self):\n",
        "        # simple getter\n",
        "        return self.gradients\n",
        "\n",
        "    def conv_activations(self, x):\n",
        "        # return conv activations for gradcam\n",
        "        return self.features_conv(x)\n",
        "\n",
        "\n",
        "# Verify our Wrapper still works as expected\n",
        "resnet_wrapper = ResNetWrapper()\n",
        "prediction = predict_model(test_image, m=resnet_wrapper)\n",
        "print_class_predicitons(prediction, top_n=10)"
      ],
      "metadata": {
        "id": "R-CjdTHDNbnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def cam_fn(_pred, class_id, _img):\n",
        "    \"\"\"Adapted from https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\"\"\"\n",
        "    pred = _pred.clone()\n",
        "\n",
        "    # Pool Gradients\n",
        "    pred[class_id] # TODO call the backward function and retain the computational graph\n",
        "    gradients = # TODO call the activations gradient function from our wrapper\n",
        "    pooled_gradients = # TODO mean the gradients using dim=[0, 2, 3]\n",
        "\n",
        "    # Weight Activation by Gradient\n",
        "    activations = # TODO call the conv activation function on _img and detach it\n",
        "    for i in range(activations.shape[1]):\n",
        "        activations[:, i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= torch.max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "\n",
        "# Get GradCAMs for most and least relevant classes\n",
        "batch = load_as_batch(test_image)\n",
        "classes = top_n_classes(prediction, n=1000)\n",
        "cam_class_0 = cam_fn(prediction, classes[0], batch).squeeze()\n",
        "cam_class_1 = cam_fn(prediction, classes[-1], batch).squeeze()\n",
        "\n",
        "# Show GradCAM\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "fig.suptitle(\"GradCAM\")\n",
        "axs[0].matshow(cam_class_0)\n",
        "axs[0].set_title(weights.meta[\"categories\"][classes[0]])\n",
        "axs[1].matshow(cam_class_1)\n",
        "axs[1].set_title(weights.meta[\"categories\"][classes[-1]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QI9DBmzBO3Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "def overlay_cam(cam, img=test_image):\n",
        "    # Bilinear Interpolation to get small CAM to same size as Image\n",
        "    cam = cv2.resize(cam, img.size)\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_VIRIDIS)\n",
        "    image_with_heatmap = cv2.addWeighted(np.array(img), 0.5, heatmap, 0.5, 0)\n",
        "    return image_with_heatmap\n",
        "\n",
        "\n",
        "overlay_0 = overlay_cam(cam_class_0)\n",
        "overlay_1 = overlay_cam(cam_class_1)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(overlay_0)\n",
        "axs[1].imshow(overlay_1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pPbV7rMXO3Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Visualization\n",
        "\n",
        "\n",
        "For Feature Visualization, we want to generate an Image, maximizing the Activation of a specific Neuron / Layer or even Class.\n",
        "\n",
        "To do this, we start from a randomly noised image."
      ],
      "metadata": {
        "id": "_bCrSrOugwM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_transform(_img):\n",
        "    \"\"\"Inverse Trnasform network input back to image space\"\"\"\n",
        "    img = _img.squeeze().detach()\n",
        "    mean = torch.tensor(weights.transforms().mean)[:, None, None]\n",
        "    std = torch.tensor(weights.transforms().std)[:, None, None]\n",
        "    img = img * std + mean\n",
        "    img = torch.swapaxes(img, 0, -1)\n",
        "    return img\n",
        "\n",
        "\n",
        "# Create Random Noise Map like an Input Image\n",
        "batch = load_as_batch(test_image)\n",
        "noise_base = torch.rand_like(batch)\n",
        "print(noise_base.shape)\n",
        "\n",
        "# Visualize the ranom noise\n",
        "img = inverse_transform(noise_base)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SrQ1oyEhkcsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create small (Sequential) networks, which correspond to our Resnet up until the 2nd to last and last convolution layer.\n",
        "\n",
        "Note: This formulation is computationally highly inefficient and requires multiple network passes. Ideally we would save the network activations during a single forward pass."
      ],
      "metadata": {
        "id": "wa9oS5Tx__3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "act_1 = torch.nn.Sequential(*list(model.children())[:-4])\n",
        "act_2 = torch.nn.Sequential(*list(model.children())[:-3])\n",
        "act_3 = torch.nn.Sequential(*list(model.children())[:-2])\n",
        "act_1(batch).shape, act_2(batch).shape, act_3(batch).shape"
      ],
      "metadata": {
        "id": "kFwz6dJ-H2FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "\n",
        "def optimize_activation(noise_base_, max_iter=100, lr=0.2):\n",
        "    noise_base = noise_base_.clone()\n",
        "    losses = []\n",
        "    for _ in tqdm.tqdm(range(max_iter)):\n",
        "        noise_base.requires_grad_()\n",
        "        loss = 0\n",
        "\n",
        "        # Very Inefficient to do Multiple runs\n",
        "        for act in [act_1]: # TODO modifiy this list to also include other layers\n",
        "            activations = act(noise_base)\n",
        "            # MAE, but MSE might also be interesting\n",
        "            loss += # TODO add the negative mean of the absolute values of the activations\n",
        "        losses.append(loss.item())\n",
        "        grad = torch.autograd.grad(loss, noise_base)[0]\n",
        "\n",
        "        # Normalize Gradients\n",
        "        noise_base.data += lr * grad / (torch.std(grad) + 1e-8)\n",
        "    return noise_base, losses\n",
        "\n",
        "\n",
        "batch = load_as_batch(test_image)\n",
        "noise_base = torch.rand_like(batch)\n",
        "\n",
        "optimized_noise_base, losses = optimize_activation(noise_base)\n",
        "\n",
        "optimized_dog, losses = optimize_activation(batch)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()\n",
        "\n",
        "img = inverse_transform(optimized_noise_base)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img = inverse_transform(optimized_dog)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WiMHuJFY2Xku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer and Attention\n",
        "\n",
        "We use a copy of https://colab.research.google.com/github/kevinzakka/clip_playground/blob/main/CLIP_GradCAM_Visualization.ipynb to work with OpenAI's CLIP Transformer and Highlight the Attention Mapping.\n",
        "\n",
        "The CLIP Model specialty is the shared latent space for image and text encoding, allowing us to investigate, which parts of an image strongly align with a given caption."
      ],
      "metadata": {
        "id": "VgksoqZ-B41B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dependencies\n",
        "# !pip install ftfy regex tqdm matplotlib opencv-python scipy scikit-image\n",
        "!pip install git+https://github.com/openai/CLIP.git -q\n",
        "\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "from PIL import Image\n",
        "from scipy.ndimage import filters\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "I3sCavbEB7Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions\n",
        "We define some helper functions."
      ],
      "metadata": {
        "id": "16wZFUGqBqsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x: np.ndarray) -> np.ndarray:\n",
        "    # Normalize to [0, 1].\n",
        "    x = x - x.min()\n",
        "    if x.max() > 0:\n",
        "        x = x / x.max()\n",
        "    return x\n",
        "\n",
        "\n",
        "# Modified from: https://github.com/salesforce/ALBEF/blob/main/visualization.ipynb\n",
        "def getAttMap(img, attn_map, blur=True):\n",
        "    if blur:\n",
        "        attn_map = filters.gaussian_filter(attn_map, 0.02 * max(img.shape[:2]))\n",
        "    attn_map = normalize(attn_map)\n",
        "    cmap = plt.get_cmap(\"jet\")\n",
        "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
        "    attn_map = (\n",
        "        1 * (1 - attn_map**0.7).reshape(attn_map.shape + (1,)) * img\n",
        "        + (attn_map**0.7).reshape(attn_map.shape + (1,)) * attn_map_c\n",
        "    )\n",
        "    return attn_map\n",
        "\n",
        "\n",
        "def viz_attn(img, attn_map, blur=True):\n",
        "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(img)\n",
        "    axes[1].imshow(getAttMap(img, attn_map, blur))\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_image(img_path, resize=None):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    if resize is not None:\n",
        "        image = image.resize((resize, resize))\n",
        "    return np.asarray(image).astype(np.float32) / 255.0"
      ],
      "metadata": {
        "id": "4SDvnscaWSZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GradCAM: Gradient-weighted Class Activation Mapping\n",
        "\n",
        "This gradCAM implementation registers a forward hook on the model at the specified layer. This allows us to save the intermediate activations and gradients at that layer.\n",
        "\n",
        "To visualize which parts of the image activate for a given caption, we use the caption as the target label and backprop through the network using the image as the input. In the case of CLIP models with resnet encoders, we save the activation and gradients at the layer before the attention pool, i.e., layer4.\n"
      ],
      "metadata": {
        "id": "mTu3PpcLBdyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hook:\n",
        "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
        "\n",
        "    def __init__(self, module: nn.Module):\n",
        "        self.data = None\n",
        "        self.hook = module.register_forward_hook(self.save_grad)\n",
        "\n",
        "    def save_grad(self, module, input, output):\n",
        "        self.data = output\n",
        "        output.requires_grad_(True)\n",
        "        output.retain_grad()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
        "        self.hook.remove()\n",
        "\n",
        "    @property\n",
        "    def activation(self) -> torch.Tensor:\n",
        "        return self.data\n",
        "\n",
        "    @property\n",
        "    def gradient(self) -> torch.Tensor:\n",
        "        return self.data.grad\n",
        "\n",
        "\n",
        "# Reference: https://arxiv.org/abs/1610.02391\n",
        "def gradCAM(\n",
        "    model: nn.Module, input: torch.Tensor, target: torch.Tensor, layer: nn.Module\n",
        ") -> torch.Tensor:\n",
        "    # Zero out any gradients at the input.\n",
        "    if input.grad is not None:\n",
        "        input.grad.data.zero_()\n",
        "\n",
        "    # Disable gradient settings.\n",
        "    requires_grad = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        requires_grad[name] = param.requires_grad\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "    # Attach a hook to the model at the desired layer.\n",
        "    assert isinstance(layer, nn.Module)\n",
        "    with Hook(layer) as hook:\n",
        "        # Do a forward and backward pass.\n",
        "        output = model(input)\n",
        "        output.backward(target)\n",
        "\n",
        "        grad = hook.gradient.float()\n",
        "        act = hook.activation.float()\n",
        "\n",
        "        # Global average pool gradient across spatial dimension\n",
        "        # to obtain importance weights.\n",
        "        alpha = grad.mean(dim=(2, 3), keepdim=True)\n",
        "        # Weighted combination of activation maps over channel\n",
        "        # dimension.\n",
        "        gradcam = torch.sum(act * alpha, dim=1, keepdim=True)\n",
        "        # We only want neurons with positive influence so we\n",
        "        # clamp any negative ones.\n",
        "        gradcam = torch.clamp(gradcam, min=0)\n",
        "\n",
        "    # Resize gradcam to input resolution.\n",
        "    gradcam = F.interpolate(\n",
        "        gradcam, input.shape[2:], mode=\"bicubic\", align_corners=False\n",
        "    )\n",
        "\n",
        "    # Restore gradient settings.\n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad_(requires_grad[name])\n",
        "\n",
        "    return gradcam"
      ],
      "metadata": {
        "id": "u4Ry-U4GVcJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #### Image & Caption settings\n",
        "image_url = \"https://images2.minutemediacdn.com/image/upload/c_crop,h_706,w_1256,x_0,y_64/f_auto,q_auto,w_1100/v1554995050/shape/mentalfloss/516438-istock-637689912.jpg\"  # @param {type:\"string\"}\n",
        "\n",
        "image_caption = \"a cat and a dog are laying on the floor\"  # @param {type:\"string\"}\n",
        "# @markdown ---\n",
        "# @markdown #### CLIP model settings\n",
        "clip_model = \"RN50\"  # @param [\"RN50\", \"RN101\", \"RN50x4\", \"RN50x16\"]\n",
        "saliency_layer = \"layer4\"  # @param [\"layer4\", \"layer3\", \"layer2\", \"layer1\"]\n",
        "# @markdown ---\n",
        "# @markdown #### Visualization settings\n",
        "blur = True  # @param {type:\"boolean\"}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(clip_model, device=device, jit=False)\n",
        "\n",
        "# Download the image from the web.\n",
        "image_path = \"image.png\"\n",
        "urllib.request.urlretrieve(image_url, image_path)\n",
        "\n",
        "image_input = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "image_np = load_image(image_path, model.visual.input_resolution)\n",
        "text_input = clip.tokenize([image_caption]).to(device)\n",
        "\n",
        "attn_map = gradCAM(\n",
        "    model.visual,\n",
        "    image_input,\n",
        "    model.encode_text(text_input).float(),\n",
        "    getattr(model.visual, saliency_layer),\n",
        ")\n",
        "attn_map = attn_map.squeeze().detach().cpu().numpy()\n",
        "\n",
        "viz_attn(image_np, attn_map, blur)"
      ],
      "metadata": {
        "id": "RPhhBLTLUxOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}