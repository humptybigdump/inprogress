{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exercise 1 Non-Maximum Supression in object detection\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"doc/sayit.jpg\" />\n",
    "</p>\n",
    "\n",
    "### Befor we start with today's exercise please read the following materials.\n",
    "\n",
    "\n",
    "## What is YOLO\n",
    "\n",
    "YOLO, short for \"You Only Look Once,\" is a groundbreaking object detection algorithm renowned for its real-time processing speed and high accuracy. Unlike traditional methods, YOLO approaches object detection as a single regression problem, directly predicting bounding boxes and class probabilities from entire images in one evaluation. Specifically, the YOLO algorithm takes an image as input and then uses a simple deep convolutional neural network to detect objects in the image. Following a fundamentally different approach to object detection, YOLO achieved state-of-the-art results, beating other real-time object detection algorithms by a large margin.\n",
    "\n",
    "## Grid Cell\n",
    "\n",
    "YOLO divides an input image into an S × S **grid**. If the center of an object bounding box falls into a **grid cell**, that **grid cell** is responsible for detecting that object. Each grid cell predicts B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and how accurate it thinks the predicted box is.\n",
    "\n",
    "<p align=\"center\" width=\"300\" height=\"200\">\n",
    "  <img src=\"doc/yolo.png\" width=\"600\" height=\"400\" alt=\"Yolo Pipeline\">\n",
    "</p>\n",
    "\n",
    "YOLO predicts multiple bounding boxes per **grid cell** based on the **anchor boxes**. At training time, we only want one bounding box predictor to be responsible for each object. YOLO assigns one predictor to be **responsible** for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at forecasting certain sizes, aspect ratios, or classes of objects, improving the overall recall score.\n",
    "\n",
    "We'll know the concept of **anchor boxes** in the next section.\n",
    "\n",
    "\n",
    "## Anchor boxes\n",
    "\n",
    "<p align=\"center\" width=\"30\" height=\"200\">\n",
    "  <img src=\"doc/anchor.png\" width=\"600\" height=\"200\" alt=\"Description\"/>\n",
    "</p>\n",
    "\n",
    "Anchor boxes, also known as anchor priors or default boxes, are pre-defined bounding boxes with specific sizes, aspect ratios, and positions that are used as reference templates during object detection. These anchor boxes are placed at each grid cell, to capture objects of different scales and shapes. During training and inference, anchor boxes are used to predict the locations and shapes of objects relative to these reference boxes.\n",
    "\n",
    "During training, the ground truth bounding boxes are assigned to the anchor boxes based on their **IoU (Intersection over Union)** overlap. Each anchor box is responsible for predicting the object whose ground truth box has the highest IOU with the anchor.\n",
    "\n",
    "Again IoU calculates the overlap between two bounding boxes by dividing the area of their intersection by the area of their union (as in the following illustration):\n",
    "\n",
    "<p align=\"center\" >\n",
    "  <img src=\"doc/IOU.png\" width=\"250\" height=\"250\" alt=\"Description\"/>\n",
    "</p>\n",
    "\n",
    "Anchor boxes help stabilize the training process by providing a consistent set of reference bounding boxes for prediction. Without anchor boxes, the model might struggle to learn meaningful bounding box predictions, especially when objects vary significantly in size and aspect ratio.\n",
    "\n",
    "**In a word, **grid cells** support the localization of predictions and anchor boxes serve the shape of predictions. We are estimating shifts of predictions wrt. certain **grid cells** and **resizes** of them wrt. certain anchor boxes**. \n",
    "\n",
    "Specifically, here is the illustration:\n",
    "\n",
    "<p align=\"center\" width=\"200\" height=\"150\">\n",
    "  <img src=\"doc/estimate.png\" width=\"500\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "where σ represents the Sigmoid function, which limit the shift ration `σ(t)` between 0 and 1 so that the middle point can only lie in the grid cell. the term `e^(t_w)` and `e^(t_h)` are the ratios between predicted bounding box sizes `(b_w, b_h)` and anchor box sizes `(p_w, p_h)`. So the network estimates are `(t_x, t_y, t_w, t_h)`.\n",
    "\n",
    "Raw Output Shape of YOLO:\n",
    "\n",
    "The output of the network gives tensors with the shape of:\n",
    "\n",
    "```(batch_size, num_anchor_box_per_cell, grid_cell_num_w, grid_cell_num_h,  data)```\n",
    "\n",
    "The `data` consists of following terms: \n",
    "\n",
    "1. The shape information `(t_x, t_y, t_w, t_h)`, \n",
    "\n",
    "2. Objectness, namely the probability of whether an object exists in the box `Pr(there_is_an_object)` and \n",
    "\n",
    "3. Conditional probability of each class `Pr(c_i|there_is_an_object)`\n",
    "\n",
    "**So in total there will be  num_anchor_box_per_cell * grid_cell_num_w * grid_cell_num_h bounding boxes as the output of the network.** \n",
    "\n",
    "While obviously, most of them are redundant (See YOLO pipeline, the upper middle image) due to: \n",
    "\n",
    "1. Low probability, no objects in the box \n",
    "\n",
    "2. Duplicated boxes that are refering the same objects while overlapping with each other. \n",
    "\n",
    "As can see in the following pictures, here all the bounding boxes are visualized in the left image. Only filtering out low probability boxes (middle) is not sufficient due to remaining boxes which overlapp.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <img src=\"doc/no_nms.jpg\" width=\"450\" height=\"600\" alt=\"No NMS\">\n",
    "  <img src=\"doc/filter_low.jpg\" width=\"450\" height=\"600\" alt=\"Description 1\">\n",
    "  <img src=\"doc/nms.jpg\" width=\"450\" height=\"600\" alt=\"Description 3\">\n",
    "</div>\n",
    "\n",
    "So a post-processing step is necessary to filter out these boxes and finally get clean predictions (right).\n",
    "\n",
    "\n",
    "## Non-Maximum Suppression (NMS)\n",
    "\n",
    "\n",
    "One key technique used in the YOLO models is **non-maximum suppression (NMS)**. NMS is a post-processing step that is used to improve the accuracy and efficiency of object detection. In object detection, it is common for multiple bounding boxes to be generated for a single object in an image. These bounding boxes may overlap or be located at different positions, but they all represent the same object. NMS is used to identify and remove redundant or incorrect bounding boxes and to output a single bounding box for each object in the image.\n",
    "\n",
    "NMS including follwing steps:\n",
    "\n",
    "1. NMS begins by setting a threshold for the confidence scores (objectness). Bounding boxes with confidence scores below this threshold are discarded as they are considered not significant or reliable.\n",
    "\n",
    "2. For the remaining bounding boxes, NMS identifies pairs of boxes that have a significant overlap, typically measured using IoU. \n",
    "\n",
    "3. Among the overlapping bounding boxes, NMS retains the one with the highest confidence score and suppresses (removes) the others. This process ensures that each detected object is represented by only one bounding box with the highest confidence score.\n",
    "\n",
    "The NMS algorithm is described in following pseudo code:\n",
    "\n",
    "<p align=\"center\" width=\"30\" height=\"200\">\n",
    "  <img src=\"doc/nms_alg.png\" width=\"650\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "Here is the a video form [DeepLearningAI](https://www.youtube.com/watch?v=VAo84c1hQX8) to give you a better understanding of how NMS works.\n",
    "\n",
    "\n",
    "## YOLOv5\n",
    "In this exercise, we use YOLOv5 from [ultralytics](https://github.com/ultralytics/yolov5), a general object detection toolbox for apply YOLO model series. The network architecture of YOLOv5 looks as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"doc/YOLOv5.png\"/>\n",
    "</p>\n",
    "\n",
    "As one of the advanced object detectors, from YOLOv3 on they usually have 3 prediction heads where different specification of grids are set up. In this way, the higher the grid number (the smaller the grid size), the smaller the objects that will be responsible for the head to detect, and vice versa. In another word, our network can detect 3 scales of objects from small to large.\n",
    "\n",
    "### In this exercise, our task is to construct the inference pipeline of YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install and import the relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2024-4-22 Python-3.11.9 torch-2.6.0+cpu CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete  (12 CPUs, 31.7 GB RAM, 277.6/463.9 GB disk)\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "import random\n",
    "import torch, torchvision\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import utils\n",
    "from utils.general import scale_boxes, xywh2xyxy\n",
    "from ultralytics.utils.plotting import Annotator, Colors\n",
    "display = utils.notebook_init() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify the parameters for initializing our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=\"./yolov5n.pt\"  # model path\n",
    "source=\"./data/images/\"  # source of images\n",
    "save_dir = \"./data/preds/\" # save images with prediction results\n",
    "imgsz=(640, 640)  # inference size (height, width)\n",
    "device=\"cpu\"  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "hide_labels=False  # hide labels\n",
    "hide_conf=False  # hide confidences\n",
    "bs=1 # batch size\n",
    "colors = Colors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Finish the pre-processing step before the image is put into the network.\n",
    "\n",
    "Rather than directly resizing a non-square image to desired size, our typical approach involves resizing it to maintain the same width-to-height ratio as the desired square image, with the larger dimension set to the desired square size. Following this, we pad the smaller dimension with black borders (as following example).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"doc/pad.png\" alt=\"Yolo Pipeline\">\n",
    "</p>\n",
    "\n",
    "For example, if we have an image with dimensions of 1280 (width) by 640 (height), and the desired shape is 480 by 480, we first resize it to 480 by 240, and then add padding of 120 pixels to each border in the height direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(im, new_shape):\n",
    "    \"\"\"Preprocess image\n",
    "\n",
    "    Args:\n",
    "        im0 (np.array): image with shape (height, width, 3) \n",
    "        new_shape (list): desired output shape [height, width]\n",
    "\n",
    "    Returns:\n",
    "        torch.tensor: image tensor with (3, height, width) normalized to 0-1\n",
    "    \"\"\"\n",
    "    # Compute padding\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[0] - new_unpad[0], new_shape[1] - new_unpad[1]  # wh padding\n",
    "    dw, dh = np.mod(dw, 32), np.mod(dh, 32)  # wh padding\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "    \n",
    "    im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0,0,0))  # add border\n",
    "    \n",
    "    im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "    im = np.ascontiguousarray(im)  # contiguous\n",
    "    \n",
    "    im = torch.from_numpy(im).to(device).float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Finish the following Non-Maximum Suppression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "    prediction,\n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.45,\n",
    "    max_det=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Non-Maximum Suppression (NMS) on inference results to reject overlapping detections.\n",
    "\n",
    "    Parameters:\n",
    "    - prediction : Tensor of shape [batch_size, num_predictions, 5+num_classes]\n",
    "                    Contains the bbox coords (x, y, w, h), confidence score and class scores.\n",
    "    - conf_thres : float, threshold for confidence score\n",
    "    - iou_thres  : float, Intersection Over Union threshold for deciding overlap\n",
    "    - max_det    : int, maximum number of detections per image\n",
    "\n",
    "    Returns:\n",
    "    - output : List of tensors, each tensor is (n,6) for each image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "\n",
    "    output = [torch.zeros((0, 6), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        x = x[xc[xi]]  # objectness score > conf_thres\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "        x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        i = i[:max_det]  # limit detections\n",
    "\n",
    "        output[xi] = x[i]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try on with your pipeline. You can store any image under `data/images` (jpg or png format) and the pipeline will randomly select one image from this path and show you the detection results\n",
    "\n",
    "Have fun with it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './yolov5n.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat() \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Model compatibility updates\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# model in eval mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './yolov5n.pt'"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = torch.load(weights, map_location=\"cpu\", weights_only=False)[\"model\"].to(device).float() \n",
    "# Model compatibility updates\n",
    "model= model.eval()  # model in eval mode\n",
    "stride = model.stride\n",
    "names = model.names\n",
    "new_shape = [640, 640]  # check image size\n",
    "\n",
    "paths = glob.glob(source + \"*\")\n",
    "random.shuffle(paths)\n",
    "path = paths[0]\n",
    "im0 = cv2.imread(path)\n",
    "\n",
    "# Run inference\n",
    "path = Path(path)\n",
    "im = im0.copy()\n",
    "im = pre_process(im, new_shape)\n",
    "# Inference\n",
    "visualize = Path(save_dir) / path.stem\n",
    "pred = model(im, visualize=False)\n",
    "# NMS\n",
    "pred = non_max_suppression(pred[0])\n",
    "s = \"\"\n",
    "\n",
    "for i, det in enumerate(pred):  # per image\n",
    "    im0 = im0.copy()\n",
    "    save_path = str(Path(save_dir) / path.name)  # im.jpg\n",
    "    \n",
    "    annotator = Annotator(im0, line_width=3, example=str(names))\n",
    "    if len(det):\n",
    "        # Rescale boxes from img_size to im0 size\n",
    "        det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "        # Print results\n",
    "        for c in det[:, 5].unique():\n",
    "            n = (det[:, 5] == c).sum()  # detections per class\n",
    "            s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "        # Write results\n",
    "        for *xyxy, conf, cls in reversed(det):\n",
    "            # Add bbox to image\n",
    "            c = int(cls)  # integer class\n",
    "            label = None if hide_labels else (names[c] if hide_conf else f\"{names[c]} {conf:.2f}\")\n",
    "            annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "    im0 = annotator.result()\n",
    "    cv2.imwrite(save_path, im0)\n",
    "\n",
    "print(s)\n",
    "display.Image(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
