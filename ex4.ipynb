{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7290b04d",
      "metadata": {
        "id": "7290b04d"
      },
      "source": [
        "# Exercise 4: U-Net for image segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb668f3",
      "metadata": {
        "id": "9fb668f3"
      },
      "source": [
        "In this exercise we want to implement a U-Net for image segmentation from lecture 2.\n",
        "U-Net is a convolutional neural network architecture designed for image segmentation, particularly in biomedical applications. It consists of two parts\n",
        "1. Encoder (contracting path): Captures context through convolution and pooling layers, reducing spatial dimensions.\n",
        "2. Decoder (expanding path): Restores spatial resolution using upsampling and combines it with corresponding encoder features via skip connections.\n",
        "\n",
        "We will segment images from the OxfordIIIPet Dataset, which contains images of different cats and dogs. More specifically, we will segment them into three parts. For each pixel the U-Net should predict if it is background the border of the pet or inside the pet. An example can be seen in the image below.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"example.png\" alt=\"example-pet\" width=\"600\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e0d2c1",
      "metadata": {
        "id": "70e0d2c1"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join, splitext\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import utils\n",
        "from oxford_pet_wrapper import OxfordPetDataset\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697038dc",
      "metadata": {},
      "source": [
        "## Configurations\n",
        "In the following cell you can see all the configurations we later use for training the U-net. Feel free to change the hyperparameters and see how they influence training. If you want to see if your code works, without performing a long training, you can also lower the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7373d21",
      "metadata": {
        "id": "d7373d21"
      },
      "outputs": [],
      "source": [
        "MODEL_CHECKPOINT_DIR = \"model_weights\"\n",
        "# define the validation percentage\n",
        "VAL_PERCENT = 0.1\n",
        "# batch size for training\n",
        "BATCH_SIZE = 32\n",
        "# learning rate for the optimizer\n",
        "LEARNING_RATE = 1e-5\n",
        "# momentum for the optimizer\n",
        "MOMENTUM = 0.999\n",
        "# gradient clipping value (for stability while training)\n",
        "GRADIENT_CLIPPING = 1.0\n",
        "# weight decay (L2 regularization) for the optimizer\n",
        "WEIGHT_DECAY = 1e-8\n",
        "# number of epochs for training\n",
        "EPOCHS = 100\n",
        "# set device to 'cuda' if CUDA is available, 'cpu' otherwise for model training and testing\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73b8e135",
      "metadata": {},
      "source": [
        "# Data Preperations\n",
        "\n",
        "In our task we will segment images from the OxfordPetIII Dataset, which contains pictures of different dogs and cats, as well as their segmentation masks. For this we first fetch the data from the internet using PyTorch. Then we preprocess the data to make it usable for our task.\n",
        "\n",
        "First of all, the images in the dataset can have different sizes, while the U-net expects inputs of the same size. Thus, we have to resize the images and their segmentation masks. Moreover, we have to normalize the images, and change the mask to mirror our three classes: background, pet, and border."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675d3834",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create parent folder to store all the results\n",
        "if not os.path.exists(\"output\"):\n",
        "    os.makedirs(\"output\")\n",
        "\n",
        "print(\"[INFO] Fetching the Oxford IIIT Pet Dataset from cache or downloading it\")\n",
        "# load the dataset\n",
        "dataset = datasets.OxfordIIITPet(\n",
        "    root=\"data\", target_types=\"segmentation\", download=True\n",
        ")\n",
        "# define the paths to the images and segmentation maps directories\n",
        "images_dir = \"./data/oxford-iiit-pet/images\"\n",
        "mask_dir = \"./data/oxford-iiit-pet/annotations/trimaps\"\n",
        "print(\"[INFO] Preparing the dataset for training\")\n",
        "# initialize the OxfordPetDataset class\n",
        "dataset = OxfordPetDataset(images_dir=images_dir, mask_dir=mask_dir)\n",
        "# split into train / validation partitions\n",
        "n_val = int(len(dataset) * VAL_PERCENT)\n",
        "n_train = len(dataset) - n_val\n",
        "train_set, val_set = random_split(\n",
        "    dataset, [n_train, n_val], generator=torch.Generator().manual_seed(0)\n",
        ")\n",
        "# create data loaders for training and validation\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(\n",
        "    val_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798a8d16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize data\n",
        "def overlay_multiclass_mask(image, mask, class_colors, alpha=0.5):\n",
        "            \"\"\"\n",
        "            Overlay multi-class mask on image.\n",
        "            - image: H x W x 3 float image (0-1)\n",
        "            - mask: H x W int mask with values 0..N classes\n",
        "            - class_colors: dict[class_value] = (r,g,b)\n",
        "            - alpha: transparency for all classes\n",
        "            Returns an image with overlays.\n",
        "            \"\"\"\n",
        "            overlay = image.copy()\n",
        "            for class_val, color in class_colors.items():\n",
        "                class_mask = (mask == class_val)\n",
        "                color_arr = np.array(color).reshape(1, 1, 3)\n",
        "                class_mask_3d = np.repeat(class_mask[:, :, np.newaxis], 3, axis=2)\n",
        "                overlay = np.where(\n",
        "                    class_mask_3d,\n",
        "                    (1 - alpha) * overlay + alpha * color_arr,\n",
        "                    overlay)\n",
        "            return overlay\n",
        "\n",
        "class_colors = {\n",
        "    0: (1, 0, 0),  # background red\n",
        "    1: (0, 1, 0),  # pet green\n",
        "    2: (0, 0, 1),  # border blue\n",
        "}\n",
        "\n",
        "fig, axs = plt.subplots(10, 2, figsize=(8, 40))\n",
        "\n",
        "for i in range(10):\n",
        "    idx = i * 80\n",
        "    sample_image = dataset[idx]['image'].detach().cpu().numpy()\n",
        "    sample_image = sample_image.transpose(1, 2, 0)\n",
        "    sample_true_mask = dataset[idx]['mask'].detach().cpu().numpy()\n",
        "    true_overlay = overlay_multiclass_mask(sample_image, sample_true_mask, class_colors, alpha=0.5)\n",
        "    axs[i, 0].imshow(sample_image)\n",
        "    axs[i, 0].axis(\"off\")\n",
        "    axs[i, 1].imshow(true_overlay)\n",
        "    axs[i, 1].axis(\"off\")\n",
        "    if i == 0:\n",
        "        axs[i, 0].set_title(\"Input Image\")\n",
        "        axs[i, 1].set_title(\"True Mask Overlay\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2900316",
      "metadata": {
        "id": "d2900316"
      },
      "source": [
        "## 4.1. Implement the network architecture\n",
        "Now its your task to implement the U-net for segmentation. Below you can see a visualization of the U-net we want to implement. You can see that it consists of two main parts. The first part is the contraction part, also named encoder, in which the input is gradually downsampled to extract fine-to-coarse features. The second part is the expansion part, also named decoder, in which the transposed convolution is used to upsample the features and reconstruct the segmentation mask from them. Different to an autoencoder for example, here we also have skip connections between the levels of equal ouput height and width, to directly  use the features of each level in the reconstruction of the segmentation mask.\n",
        "\n",
        "In the visualization you can see the the number of channels as the number above each layer. And the size of the features as the tuple (axa) before each level.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"U_net.png\" alt=\"u-net\" width=\"1000\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e63dfec",
      "metadata": {},
      "source": [
        "a) Looking at the network architecture, we can see a recurring pattern. In each level we have two 2D convolutions with filter-size 3x3, and padding 1, each followed by a batch normalization and ReLU activation function. Because, we will reuse it throughout the whole network, lets first implement this block.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"U_net-convs.png\" alt=\"dualconv\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f8110e",
      "metadata": {
        "id": "81f8110e"
      },
      "outputs": [],
      "source": [
        "class DualConv(nn.Module):\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(DualConv, self).__init__()\n",
        "\n",
        "        # TODO: implement the two convolutions \n",
        "        # each followed by a batch normalization and ReLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement the forward pass\n",
        "        \n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a11dc1a9",
      "metadata": {},
      "source": [
        "b) Lets now look at the contraction part. Here we also have recurring patterns, one of which is framed turquoise in the visualization below. For each contraction, the input is first downsampled using max pooling, and then features at this next coarser level are extracted using two 2D convolutions. \n",
        "Implement one contraction, reusing the class ```DualConv``` you implemented in (a).\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"U_net-encoder.png\" alt=\"unet-encoder\" height=\"500\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd0f9ac2",
      "metadata": {
        "id": "bd0f9ac2"
      },
      "outputs": [],
      "source": [
        "class Contract(nn.Module):\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(Contract, self).__init__()\n",
        "\n",
        "        # TODO: implement a single contraction\n",
        "\n",
        "    def forward(self, x):\n",
        "         # TODO: implement the forward pass\n",
        "         \n",
        "         return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3616f3a",
      "metadata": {},
      "source": [
        "c) Analogously to the one contraction, we implemented in (b), lets now impelement one expansion block. Here, we first upsample using the Transposed Convolution. The transposed convolution should half the number of input channels and double the feature size. For this, we use a 2x2 kernel and a stride of 2. To get a better intuition of what this does, you can also look at the following visualization [here](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md). After upsampling again two 2D convolutions are applied, for which we can reuse the class ```DualConv```.\n",
        "When looking at the skeleton code below, you might realize that the forward method, gets two inputs ```x1``` and ```x2```. This is because we have the skip connections as a second input to the block.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"U_net-decoder.png\" alt=\"unet-decoder\" height=\"500\"/>\n",
        "</p>\n",
        "\n",
        "Below a visualization of the transposed convolution with a stride of 2. Notice that the stride is not applied on the input channel, but rather when creating the outpu.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"ConvTransposed.png\" alt=\"Trasnposed Convolution with stride 2\" height=\"400\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9223bc86",
      "metadata": {
        "id": "9223bc86"
      },
      "outputs": [],
      "source": [
        "class Expand(nn.Module):\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(Expand, self).__init__()\n",
        "        \n",
        "        # TODO: impelement a single expansion\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "\n",
        "        # TODO: implement the forward pass\n",
        "        # x1 is the input from the lower level to be upsampled\n",
        "        # x2 is the input from the skip connection\n",
        "        \n",
        "        # TODO: upsample input from lower level\n",
        "        x_up = \n",
        "        \n",
        "        # pad the upsampled features (output of Transpose Convolution), \n",
        "        # s.t. the feature size is the same as that of the feature size\n",
        "        #  from the input of the skip connection\n",
        "        diff_y = x2.size()[2] - x_up.size()[2]\n",
        "        diff_x = x2.size()[3] - x_up.size()[3]\n",
        "        x = F.pad(\n",
        "            x1, [diff_x // 2, diff_x - diff_x // 2, diff_y // 2, diff_y - diff_y // 2]\n",
        "        )\n",
        "\n",
        "        # TODO: concatenate channels from skip connections and channels from upsampled input \n",
        "        # afterwards #channels = #channels(x2) + #channels(x_up) \n",
        "\n",
        "        # TODO: apply convolutions to concatenated features\n",
        "        return "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f9d253",
      "metadata": {},
      "source": [
        "d) Now that we have all the building blocks of our U-Net, we can put them together to implement the whole network. In our specific implementation we have four contractions and four expansions. \n",
        "Also don't forget the initial convolutions applied to our input image and the final convolution applied to get our output mask. Because, we have three classes (background, pet, border), we need an output with expect an output with three channels. To determine the class to which the pixel belongs, we can take the argmax over the output-channels. (Hint: You should not normalize the output to be in the range of [0, 1], but can directly output the unnomralized raw logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef88b5ee",
      "metadata": {
        "id": "ef88b5ee"
      },
      "outputs": [],
      "source": [
        "class CustomUNet(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(CustomUNet, self).__init__()\n",
        "\n",
        "        # TODO: initial convolutions on input image\n",
        "\n",
        "        # TODO: contractions\n",
        "\n",
        "        # TODO: expansions\n",
        "\n",
        "        # TODO: final convolution to get output segmentation maks\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: implmenent the forward method, \n",
        "        # to pass the input image though all layers to get \n",
        "        # the final output of the U-net\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26155da4",
      "metadata": {},
      "source": [
        "## 4.1) Implement the Training Loop\n",
        "\n",
        "One key factor in training our network is deciding on the loss metric. By now you might know that for multi-class classification a commonly used loss is the Cross Entropy Loss. However, the cross entropy loss is calculates per pixel, and penalizes wrong class predictions independently of how many pixels belong to the class. Thus, it may not perform well, if we have inbalanced classes, which is the case for us. For example the border classes contain much less pixels, and the background class contains the most pixels.\n",
        "Thus, we will use a common loss for image segmentation: the Dice loss. The dice coefficient does not look at each individual pixel, but rather the  area of overlap between the ground truth and predicted class segments. It is computed for each class, and then averaged over the classes. For each class the area of intersection is computed as the sum of predicted probabilities where the pixels belong to that class. The total area is computed as the predicted probability mass plus the total number of pixels belonging to that class. The dice loss is simply (1 - dice_score). We already provide an implementation of the dice score and dice loss in ```utils.py```.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"dice.png\" alt=\"dice-score\" width=\"600\"/>\n",
        "</p>\n",
        "\n",
        "However, the dice loss is less sensitive, to errors in individual pixel misclassifications. Thus, as loss function we use the sum of the cross-entropy loss and the dice loss.\n",
        "\n",
        "<p align=\"center\">\n",
        "Loss = Cross-Entropy-Loss + Dice-Loss\n",
        "</p>\n",
        "\n",
        "Below you can see that we already implemented parts of the training loop, to visualize and track the training progress. \n",
        "Your task, is to implement the actual training of the U-net. See below, marked with a TODO what you should implement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0fa7d22",
      "metadata": {
        "id": "c0fa7d22"
      },
      "outputs": [],
      "source": [
        "# call the UNet class to initialize the model\n",
        "model = CustomUNet(input_channels=3, num_classes=3)\n",
        "model.to(device=DEVICE)\n",
        "# set up the optimizer, the categorical loss, the learning rate scheduler\n",
        "optimizer = torch.optim.RMSprop(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    momentum=MOMENTUM,\n",
        "    foreach=True,\n",
        ")\n",
        "# adaptibly reduce learning rate, if dice score stagnates over the course of five epochs\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \"max\", patience=5\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bf42870",
      "metadata": {
        "id": "3bf42870",
        "outputId": "f1e4d026-1bc0-455c-e51a-3afa85647290"
      },
      "outputs": [],
      "source": [
        "# initialize lists for storing loss and validation Dice scores over epochs\n",
        "epoch_losses = []\n",
        "val_scores = []\n",
        "train_scores = []\n",
        "val_losses = []\n",
        "print(\"[INFO] Starting training\")\n",
        "# begin training\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # set the model to training mode\n",
        "    model.train()\n",
        "    # initialize the epoch loss and epoch Dice score variables to store the loss and Dice score for each epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_dice_score = 0\n",
        "    # create a progress bar for training and wrap it with tqdm to display progress during training\n",
        "    with tqdm(total=n_train, desc=f\"Epoch {epoch}/{EPOCHS}\", unit=\"img\") as pbar:\n",
        "        # iterate over the training set\n",
        "        for batch in train_loader:\n",
        "            # extract the image and mask batch, and move the batch to the device\n",
        "            images, true_masks = batch['image'], batch['mask']\n",
        "            # move images and masks to correct device and type\n",
        "            images = images.to(\n",
        "                device=DEVICE,\n",
        "                dtype=torch.float32,\n",
        "                memory_format=torch.channels_last,\n",
        "            )\n",
        "            true_masks = true_masks.to(device=DEVICE, dtype=torch.long)\n",
        "            # predict the mask using the model\n",
        "\n",
        "            #####################################################################\n",
        "            # TODO:                                                             #\n",
        "            # - predict masks                                                   #\n",
        "            # - compute cross entropy and dice loss and sum them                #\n",
        "            # - do optimization step with computed loss                         #\n",
        "            # (don't forget to set your gradients to zero)                      #\n",
        "            # add your loss to the epoch_loss to track the models training      #\n",
        "            #####################################################################\n",
        "\n",
        "            masks_pred = \n",
        "\n",
        "            loss = \n",
        "            \n",
        "\n",
        "            # End Todo ##########################################################################\n",
        "            # update the progress bar with the loss for the current batch\n",
        "            pbar.set_postfix(**{\"loss (batch)\": loss.item()})\n",
        "            # compute Dice score for training set for this batch and add it to the epoch Dice score\n",
        "            dice_score_batch = utils.multi_class_dice_coeff(\n",
        "                true_masks, masks_pred\n",
        "            )\n",
        "            epoch_dice_score += (\n",
        "                dice_score_batch.item()\n",
        "            )  # Sum up the Dice score for each batch\n",
        "            # compute average loss and Dice score for this epoch\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    avg_dice_score = epoch_dice_score / len(train_loader)\n",
        "    # append the average loss and Dice score to the respective lists\n",
        "    epoch_losses.append(avg_loss)\n",
        "    train_scores.append(avg_dice_score)\n",
        "    # print the average loss and Dice score for this epoch\n",
        "    print(\n",
        "        f\"[INFO] Epoch {epoch} finished! Loss: {avg_loss}, Train Dice Score: {avg_dice_score}\"\n",
        "    )\n",
        "    # evaluation at the end of the epoch on the validation set\n",
        "    val_score, val_loss = utils.evaluate(\n",
        "        model, val_loader, DEVICE, criterion=criterion\n",
        "    )\n",
        "    # update the learning rate scheduler based on the validation Dice score\n",
        "    scheduler.step(val_score)\n",
        "    # print the validation loss and Dice score for this epoch\n",
        "    print(f\"[INFO] Validation Loss: {val_loss}, Validation Dice score: {val_score}\")\n",
        "    # append the validation loss and Dice score to the respective lists\n",
        "    val_losses.append(val_loss)\n",
        "    val_scores.append(val_score)\n",
        "    clear_output(wait=True)\n",
        "    utils.plot_training(epoch_losses=epoch_losses, val_scores=val_scores, train_scores=train_scores, val_losses=val_losses)\n",
        "\n",
        "    utils.visualize_predictions(model=model, dataloader=train_loader, device=DEVICE)\n",
        "    \n",
        "    # save the model checkpoint after each epoch\n",
        "    Path(\"output\", MODEL_CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    state_dict = model.state_dict()\n",
        "    state_dict[\"mask_values\"] = dataset.mask_values\n",
        "    # construct the path for saving the checkpoint\n",
        "    checkpoint_path = os.path.join(\n",
        "        \"output\", MODEL_CHECKPOINT_DIR, f\"checkpoint_epoch{epoch}.pth\"\n",
        "    )\n",
        "    torch.save(state_dict, checkpoint_path)\n",
        "    print(f\"[INFO] Checkpoint {epoch} saved at: {checkpoint_path}\")\n",
        "print(\n",
        "    \"[INFO] Training is completed\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ex-mlrs2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
