{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b11698e",
   "metadata": {},
   "source": [
    "# MLRS2 Exercise - Tabular Q-Learning\n",
    "\n",
    "In this exercise, you will implement the basic Q-Learning algorithm for a grid world environment.\n",
    "First, let's install and import the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a48ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa716253",
   "metadata": {},
   "source": [
    "Now we introduce a helper function which visualizes the current Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grid(env, q_table, fig, ax, path=[]):\n",
    "    v_table = np.max(q_table, axis=-1)\n",
    "    action = np.argmax(q_table, axis=-1)\n",
    "\n",
    "    action_symbs = [\"^\", \"v\", \"<\", \">\"]\n",
    "\n",
    "    for x in range(env.size):\n",
    "        for y in range(env.size):\n",
    "            if (x, y) == env.goal:\n",
    "                ax.add_patch(patches.Rectangle((y, env.size - 1 - x), 1, 1, edgecolor='black', facecolor='cyan'))\n",
    "            elif any(wx <= x < wx + wwidth and wy <= y < wy + wheight for wx, wy, wwidth, wheight in env.walls):\n",
    "                ax.add_patch(patches.Rectangle((y, env.size - 1 - x), 1, 1, edgecolor='black', facecolor='gray'))\n",
    "            elif (x, y) in path:\n",
    "                ax.add_patch(patches.Rectangle((y, env.size - 1 - x), 1, 1, edgecolor='black', facecolor='cyan'))\n",
    "            else:\n",
    "                ax.add_patch(patches.Rectangle((y, env.size - 1 - x), 1, 1, edgecolor='black', facecolor='white'))\n",
    "\n",
    "            plt.text((y + 1/4), env.size - 1 - x + 1/4, str(np.round(v_table[x * env.size + y], 2)), fontsize=16)\n",
    "            if x < env.size - 1 or y < env.size - 1:\n",
    "                plt.text((y + 2 / 5), env.size - 1 - x + 1 / 2, action_symbs[action[x * env.size + y]], fontsize=12)\n",
    "\n",
    "    agent_x, agent_y = env.state\n",
    "    ax.add_patch(patches.Rectangle((agent_y, env.size - 1 - agent_x), 1, 1, edgecolor='black', facecolor='magenta'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b046467",
   "metadata": {},
   "source": [
    "### Define Grid World Environment\n",
    "\n",
    "The `GridWorld` class defines the environment of the agent. The `step` method can be used to perform an action in the environment. The `select_action` method takes the Q-Values for a state as input and performs an epsilon-greedy action selection. With a probability of $\\epsilon$ a random action is sampled from the action space and returned, otherwise we select:\n",
    "\n",
    "$$a_t = \\max_a Q(s_t,a)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2aef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=10, epsilon=0.01, walls=[]):\n",
    "        self.size = size\n",
    "        self.goal = (0, self.size - 1)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.action_space = len(self.actions)\n",
    "        self.walls = walls\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (self.size - 1, 0)\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def is_wall(self, x, y):\n",
    "        for (wx, wy, wwidth, wheight) in self.walls:\n",
    "            if wx <= x < wx + wwidth and wy <= y < wy + wheight:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0 and x > 0 and not self.is_wall(x - 1, y):  # up\n",
    "            x -= 1\n",
    "        elif action == 1 and x < self.size - 1 and not self.is_wall(x + 1, y):  # down\n",
    "            x += 1\n",
    "        elif action == 2 and y > 0 and not self.is_wall(x, y - 1):  # left\n",
    "            y -= 1\n",
    "        elif action == 3 and y < self.size - 1 and not self.is_wall(x, y + 1):  # right\n",
    "            y += 1\n",
    "\n",
    "        self.state = (x, y)\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 1, True  # Reward of 1 for reaching the goal\n",
    "        else:\n",
    "            return self.state, -0.01, False  # Small negative reward to encourage faster solutions\n",
    "\n",
    "    def get_state_index(self, state):\n",
    "        x, y = state\n",
    "        return x * self.size + y\n",
    "\n",
    "    def select_action(self, q_values):\n",
    "        #####################################SOLUTION#######################################################################\n",
    "        # Epsilon greedy selection of action\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            action = random.choice(range(self.action_space))\n",
    "        else:\n",
    "            action = np.argmax(q_values).item()\n",
    "        #####################################SOLUTION#######################################################################\n",
    "        ############ TODO #############\n",
    "        # Implement epsilon-greedy action selection\n",
    "        ############ TODO #############\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.1\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047addb5",
   "metadata": {},
   "source": [
    "Now, define the walls within the grid world and setup the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################SOLUTION###################################################################################\n",
    "# Define walls within the gridworld\n",
    "walls = [(1, 1, 1, 3), (8, 8, 2, 1), (2, 6, 3, 2), (7, 2, 1, 2)]\n",
    "\n",
    "# Define grid world environment\n",
    "env = GridWorld(size=10, epsilon=epsilon, walls=walls)\n",
    "##################################SOLUTION##################################################################################\n",
    "######## TODO #########\n",
    "# Define walls within the gridworld\n",
    "#walls = []\n",
    "\n",
    "# Define grid world environment\n",
    "#env = ...\n",
    "######## TODO #########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6cf3f",
   "metadata": {},
   "source": [
    "Initialize the Q-table with zeros. For each state in the environment, the Q-table will hold values for all actions of the action space in that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning Training\n",
    "################################SOLUTION####################################################################################\n",
    "# Define Q-table\n",
    "q_table = np.zeros((env.size * env.size, env.action_space))\n",
    "################################SOLUTION####################################################################################\n",
    "######## TODO #########\n",
    "# Define Q-table\n",
    "#q_table = ...\n",
    "######## TODO #########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eaf20c",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm Training and Testing\n",
    "\n",
    "Implement the training loop for Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track rewards and loss\n",
    "rewards = []\n",
    "losses = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    episode_steps = 0\n",
    "    episode_reward = 0\n",
    "    episode_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        #########################################SOLUTION###################################################################\n",
    "        # Get state index\n",
    "        state_index = env.get_state_index(state)\n",
    "\n",
    "        # select action\n",
    "        action = env.select_action(q_table[state_index])\n",
    "\n",
    "        # Environment step\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state_index = env.get_state_index(next_state)\n",
    "\n",
    "        # Q-learning update\n",
    "        q_target = reward + gamma * np.max(q_table[next_state_index]).item()\n",
    "        q_table[state_index][action] = q_table[state_index][action] + learning_rate * (q_target - q_table[state_index][action])\n",
    "\n",
    "        # Calculate q loss\n",
    "        q_loss = (q_target - q_table[state_index][action]) ** 2\n",
    "\n",
    "        # Transition to next state\n",
    "        state = next_state\n",
    "        ###########################################SOLUTION#################################################################\n",
    "        ############ TODO #############\n",
    "        # Get state index\n",
    "\n",
    "        # select action\n",
    "\n",
    "        # Environment step\n",
    "\n",
    "        # Q-learning update\n",
    "\n",
    "        # Calculate q loss\n",
    "\n",
    "        # Transition to next state\n",
    "\n",
    "        ############ TODO #############\n",
    "        episode_reward += reward\n",
    "        episode_loss += q_loss.item()\n",
    "        episode_steps += 1\n",
    "\n",
    "    episode_reward /= episode_steps\n",
    "    episode_loss /= episode_steps\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    losses.append(episode_loss)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode:<3}, Episode Loss: {episode_loss:.4f}, Episode Reward: {episode_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79486f5a",
   "metadata": {},
   "source": [
    "### Test the agent\n",
    "\n",
    "Test the trained agent within the grid world to find the optimal sequence of actions to get from the start to the goal. During testing, we don't need to explore with the epsilon-greedy policy but can greedily choose the action with the highest Q-value estimate in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, q_table):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    path = [state]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.set_xlim(0, env.size)\n",
    "    ax.set_ylim(0, env.size)\n",
    "    ax.set_xticks(range(env.size + 1))\n",
    "    ax.set_yticks(range(env.size + 1))\n",
    "    ax.grid(True)\n",
    "\n",
    "    while not done:\n",
    "        ############################################SOLUTION################################################################\n",
    "        # Get state index\n",
    "        state_index = env.get_state_index(state)\n",
    "\n",
    "        # Greedily select action\n",
    "        action = np.argmax(q_table[state_index]).item()\n",
    "\n",
    "        # Perform environment step\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Transition to next state\n",
    "        state = next_state\n",
    "        #############################################SOLUTION###############################################################\n",
    "        ############ TODO #############\n",
    "        # Get state index\n",
    "\n",
    "        # Greedily select action\n",
    "\n",
    "        # Perform environment step\n",
    "\n",
    "        # Transition to next state\n",
    "        ############ TODO ##############\n",
    "        path.append(state)\n",
    "        \n",
    "\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    # Visualize grid\n",
    "    visualize_grid(env, q_table, fig, ax, path)\n",
    "\n",
    "    total_reward /= steps\n",
    "    print(f\"Reached the goal in {steps} steps with a total reward of {total_reward:.4f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4598883",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the trained agent and visualize its path\n",
    "test_agent(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f86574",
   "metadata": {},
   "source": [
    "Finally, we plot the recorded data of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed04b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots arranged horizontally\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot data in the first subplot\n",
    "ax1.plot(range(num_episodes), rewards)\n",
    "ax1.set_title('Reward over Episodes')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Reward')\n",
    "\n",
    "# Plot data in the second subplot\n",
    "ax2.plot(range(num_episodes), losses)\n",
    "ax2.set_title('Q-Loss over Episodes')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Q-Loss')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
