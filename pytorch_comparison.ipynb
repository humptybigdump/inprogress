{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "963db690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyCustomFCLayer(torch.autograd.Function):\n",
    "    # Don't actually use this in pratice, does not consider broadcasting.\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W, b=None):\n",
    "        ctx.save_for_backward(x, W)\n",
    "        print(\"Custom Forward was called!\")\n",
    "        z = torch.matmul(W, x) # Matrix multiplication\n",
    "        if b is not None:\n",
    "            z += b # Direct addition of the bias vector\n",
    "        return z\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_z):\n",
    "        x, W = ctx.saved_tensors\n",
    "        print(f\"Custom Backward was called! Got output gradient {grad_z}\")\n",
    "        grad_x = grad_W = grad_b = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_x = W.T @ grad_z # nabla_x Wx + b = W^T, nabla_x f = W^T nabla_z f\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_W = torch.outer(grad_z, x) # analytical without tensor product with sum\n",
    "        if ctx.needs_input_grad[2]:\n",
    "            grad_b = grad_z # Direct gradient without summing\n",
    "        return grad_x, grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5347db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x : tensor([0., 1., 2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_dim = 3\n",
    "z_dim = 5\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "t = torch.randn(size=(z_dim,))\n",
    "\n",
    "x_pytorch = torch.arange(x_dim, dtype=torch.float32, requires_grad=True)\n",
    "x_ours = torch.arange(x_dim, dtype=torch.float32, requires_grad=True)\n",
    "print(\"input x :\", x_ours)\n",
    "\n",
    "# pytorch W and b init\n",
    "lin = torch.nn.Linear(in_features=x_dim, out_features=z_dim)\n",
    "\n",
    "# Our manual W and b. We copy parameters for comparison\n",
    "W = torch.empty(size=(z_dim, x_dim), requires_grad=True)\n",
    "b = torch.empty(size=(z_dim,), requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    W.copy_(lin.weight)\n",
    "    b.copy_(lin.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f486712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Forward was called!\n"
     ]
    }
   ],
   "source": [
    "z_pytorch = lin(x_pytorch)\n",
    "\n",
    "z_ours = MyCustomFCLayer.apply(x_pytorch, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "159da67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our z:  tensor([-1.3940,  0.5576,  0.0218,  0.5202,  1.6055],\n",
      "       grad_fn=<MyCustomFCLayerBackward>)\n",
      "PyTorch's z:  tensor([-1.3940,  0.5576,  0.0218,  0.5202,  1.6055], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Our z: \", z_ours)\n",
    "print(\"PyTorch's z: \", z_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9b07c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Forward was called!\n",
      "Custom Backward was called! Got output gradient tensor([-1.1740,  0.3404,  0.8802, -0.0193,  1.0760])\n",
      "\n",
      "Our grad x:  tensor([0.2009, 0.3733, 1.1361])\n",
      "Our grad W:  tensor([[-0.0000, -1.1740, -2.3480],\n",
      "        [ 0.0000,  0.3404,  0.6808],\n",
      "        [ 0.0000,  0.8802,  1.7605],\n",
      "        [-0.0000, -0.0193, -0.0386],\n",
      "        [ 0.0000,  1.0760,  2.1520]])\n",
      "Our grad b:  tensor([-1.1740,  0.3404,  0.8802, -0.0193,  1.0760])\n"
     ]
    }
   ],
   "source": [
    "# Ignore this. Just to make sure that we can rerun this cell without issues\n",
    "z_ours = MyCustomFCLayer.apply(x_ours, W, b)   # Ignore this.\n",
    "x_ours.grad = W.grad = b.grad = None           # Ignore this.\n",
    "\n",
    "mse_ours = (z_ours - t).square().mean()\n",
    "mse_ours.backward()\n",
    "print()\n",
    "print(\"Our grad x: \", x_ours.grad)\n",
    "print(\"Our grad W: \", W.grad)\n",
    "print(\"Our grad b: \", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d5b1e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch grad x:  tensor([0.2009, 0.3733, 1.1361])\n",
      "PyTorch grad W:  tensor([[-0.0000, -1.1740, -2.3480],\n",
      "        [ 0.0000,  0.3404,  0.6808],\n",
      "        [ 0.0000,  0.8802,  1.7605],\n",
      "        [-0.0000, -0.0193, -0.0386],\n",
      "        [ 0.0000,  1.0760,  2.1520]])\n",
      "PyTorch grad b:  tensor([-1.1740,  0.3404,  0.8802, -0.0193,  1.0760])\n"
     ]
    }
   ],
   "source": [
    "# Ignore this. Just to make sure that we can rerun this cell without issues\n",
    "z_pytorch = lin(x_pytorch)                                  # Ignore this.\n",
    "x_pytorch.grad = lin.weight.grad = lin.bias.grad = None     # Ignore this.\n",
    "\n",
    "mse_pytorch = (z_pytorch - t).square().mean()\n",
    "mse_pytorch.backward()\n",
    "print(\"PyTorch grad x: \", x_pytorch.grad)\n",
    "print(\"PyTorch grad W: \", W.grad)\n",
    "print(\"PyTorch grad b: \", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a8ca4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlga25_exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
